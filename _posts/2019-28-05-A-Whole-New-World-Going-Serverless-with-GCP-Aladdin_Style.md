# A Whole New World: Going Serverless with GCP (Aladdin-style)

### Once upon a time, a young street urchin from a kingdom far, far away was managing virtual machines and cron jobs for a living but soon his life will turn upside down when he discovers serverless.

Having a setup where a simple script is running once a day/week/month on a server is not ideal. Sure, this could be a small, free-tier virtual machine running in a public cloud, so who cares?

In this article I will give a quick how-to for replacing a simple data retrieval pipeline (as the one described [in this article](https://tgel0.github.io/blog/spotify-data-project-part-1-from-data-retrieval-to-first-insights/)) with a completely serverless solution using the Google Cloud Platform.

Note:  there might be a reference or two from Disney's Aladdin movie which I recently watched!

Let's imagine ourselves in the ancient kingdom of [Agrabah](https://onceuponatime.fandom.com/wiki/Agrabah). The protagonist of our story was looking for a new solution for his data pipeline when a giant, blue ghost came out of a magic lamp.

![](https://media.giphy.com/media/4K3l1T9MZwtoivFqtx/giphy.gif)

## ğŸ§ Three Wishes

 

The Genie granted three wishes:

- I wish to never have to worry about my infrastructure ever again
- I wish to only pay for the compute time my code is using to run
- and finally, I wish automatic scaling with high availability

## â˜ï¸ Answer is serverless

Back in the old days, this would probably seem impossible even for Big G.

But today all the major cloud providers have some sort of serverless option: AWS Lambda, MS Azure Functions, GCP Cloud Functions.

In this project, I implemented the following architecture to get a weekly dataset of audio data from the Spotify API:

![](diagram-1dbbf33f-52bd-406f-9904-c6ea2ab56593.png)

In more detail this means:

â˜ï¸ the first [Cloud Function](https://cloud.google.com/functions/) is retrieving the data from the Spotify API and storing it as CSV

â˜ï¸  [Cloud Storage](https://cloud.google.com/storage/) is just for storing the CSVs

â˜ï¸ the second CF will load the data from the CSVs into BigQuery

â˜ï¸  [BigQuery](https://cloud.google.com/bigquery/) is our serverless data warehouse holding all the data

â˜ï¸  last but not least, [Cloud Scheduler](https://cloud.google.com/scheduler/) (which is not shown in the diagram above) will trigger the first CF once a week

## â˜ï¸Command Line vs. Web Console

Interacting with GCP services can be done via the CLI or web console (REST API being another option). Personally, I find the web console very user-friendly and can absolutely recommend it. In fact, I did this project mostly using the web frontend.

### â˜ï¸ Cloud Functions

Two Cloud Functions were needed here: one for retrieving the data from Spotify and the other one for loading the data into BigQuery.

Besides the function itself there is one major difference between them: the trigger.

The first one is going to be triggered by an HTTP request (like opening an URL with a web browser). The second one by a Cloud Storage event (whenever a new CSV is stored in the Cloud Storage bucket).

ğŸ Gotcha alert: my Cloud Function was crashing until I added a 'greater-than' sign in my `requirements.txt` file. 

So I basically changed this:

    pandas==0.22.0
    spotipy==2.4.4
    google-cloud-storage==1.15.0

Into this:

    pandas>=0.22.0
    spotipy>=2.4.4
    google-cloud-storage>=1.15.0

The second Cloud Function is going to take the CSV's from the Google Storage bucket and load them into BigQuery.

### â˜ï¸  BigQuery

Setting up a dataset and table in BigQuery is fairly easy with the the web console. Just make sure to not miss any columns or data types.

![](spotify_data_bigq-6a54b791-4bca-4b95-a3c8-c11105c18d5f.png)

### â˜ï¸  Cloud Scheduler

We could now trigger the whole data pipeline manually by opening the URL endpoint of the first Cloud Function in a web browser. But that's not how we do things *here in the kingdom of Agrabah* ğŸ§.

With the help of [Cloud Scheduler](https://cloud.google.com/scheduler/) we will automatically trigger the URL endpoint at a specified schedule.

Below is the schedule for "every Monday at 12:30 AM". Target/method needs to be HTTP/POST.

    30 00 * * 1

ğŸ Picking a schedule using the unix-cron format is another gotcha. [This website](https://crontab.guru/) can help.

After everything was set up we can just fly away on our magic carpet and let the automation do it's thing.

![](https://media.giphy.com/media/WUu9EGdSEImJy/giphy.gif)

## T H E     E N D

## References

ğŸ”— [Moving your cron job to the cloud with Google Cloud Functions](https://dev.to/di/moving-your-cron-job-to-the-cloud-with-google-cloud-functions-1ecp?__s=4oo7uhefd3rtrvzokcnh) by Dustin Ingram

ğŸ”— [Automated insert of CSV data into Bigquery via GCS bucket + Python](https://rickt.org/2018/10/22/poc-automated-insert-of-csv-data-into-bigquery-via-gcs-bucket-python/) by Rick Tait

ğŸ”— My [github repo](https://github.com/tgel0/spotify-data) for this project (includes all the notebooks, scripts and more)